{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8b18cc",
   "metadata": {},
   "source": [
    "# Rational Bubbles in NFTs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897a40a",
   "metadata": {},
   "source": [
    "## Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72f888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from statsmodels.tsa.stattools import adfuller, kpss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a517e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcoin = pd.read_csv('bcoin.csv')\n",
    "cake = pd.read_csv('cake.csv')\n",
    "brl = pd.read_csv('brl.csv')\n",
    "eur = pd.read_csv('eur.csv')\n",
    "jpy = pd.read_csv('jpy.csv')\n",
    "cny = pd.read_csv('cny.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abff8dd",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5f4b7",
   "metadata": {},
   "source": [
    "The Brl, Eur, Jpy and Cny data will have a smaller number of observations since they are not quoted on weekends or specific holidays, so I'm going to carry out the oversampling process, adjusting the smaller bases to contain NA on non-closing dates, and then carry out a missing imputation with a moving average. \n",
    "The idea of oversampling comes from the fact that the matrices need to be the same size, which is not the case, and undersampling can lead to the loss of important data for the test. The literature supports oversampling because it is easy to impute missing in these cases without biasing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d13069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfit(df, data_col='Date', valor_col='Close', formato='%d/%m/%Y'):\n",
    "\n",
    "    df[data_col] = pd.to_datetime(df[data_col], format=formato)\n",
    "    \n",
    "    min_date = df[data_col].min()\n",
    "    max_date = df[data_col].max()\n",
    "    all_dates = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "    df_c = pd.DataFrame({data_col: all_dates})\n",
    "\n",
    "    df = df_c.merge(df, on=data_col, how='left')\n",
    "\n",
    "    df = df.sort_values(data_col).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "brl = overfit(brl)\n",
    "cny = overfit(cny)\n",
    "jpy = overfit(jpy)\n",
    "eur = overfit(eur)\n",
    "\n",
    "for df in [brl, cny, jpy, eur]:\n",
    "    df['Date_str'] = df['Date'].dt.strftime('%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb5e01",
   "metadata": {},
   "source": [
    "## Missing Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa007784",
   "metadata": {},
   "source": [
    "As the dates placed through oversampling have no entries for the currencies, I'm going to perform an imputation process by local average, thus guaranteeing a “smoothness” in the imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2474195a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNY:\n",
      "  std_before : 0.0066\n",
      "  std_after  : 0.0066\n",
      "\n",
      "BRL:\n",
      "  std_before : 0.0095\n",
      "  std_after  : 0.0095\n",
      "\n",
      "JPY:\n",
      "  std_before : 0.0007\n",
      "  std_after  : 0.0007\n",
      "\n",
      "EUR:\n",
      "  std_before : 0.0492\n",
      "  std_after  : 0.0492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def loc_mean(df, value_col='Close'):\n",
    "\n",
    "    std_before = df[value_col].std()\n",
    "\n",
    "    prev = df[value_col].shift(1)\n",
    "    next_ = df[value_col].shift(-1)\n",
    "    \n",
    "    df[value_col] = df[value_col].combine_first(\n",
    "        ((prev + next_) / 2)\n",
    "    )\n",
    "    \n",
    "    std_after = df[value_col].std()\n",
    "\n",
    "    return df, std_before, std_after\n",
    "\n",
    "currencies = {'CNY': cny, 'BRL': brl, 'JPY': jpy, 'EUR': eur}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, df in currencies.items():\n",
    "    df_imputed, std_before, std_after = loc_mean(df.copy())\n",
    "    results[name] = {\n",
    "        'std_before': std_before,\n",
    "        'std_after': std_after\n",
    "    }\n",
    "\n",
    "    currencies[name] = df_imputed\n",
    "\n",
    "for name, res in results.items():\n",
    "    print(f'{name}:')\n",
    "    print(f'  std_before : {res[\"std_before\"]:.4f}')\n",
    "    print(f'  std_after  : {res[\"std_after\"]:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8dc838",
   "metadata": {},
   "source": [
    "We can notice that the change in standard deviation was very low, which according to the literature is a good indicator when performing imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf3711",
   "metadata": {},
   "source": [
    "## Unit Root Tests (URT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134b358b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRL - Level\n",
      "  ADF  : stat=-1.91, lag=3, p=0.3281\n",
      "  KPSS: stat=0.58, lag=21, p=0.0246\n",
      "\n",
      "BRL - First Difference\n",
      "  ADF  : stat=-13.57, lag=2, p=0.0000\n",
      "  KPSS: stat=0.06, lag=4, p=0.1000\n",
      "\n",
      "EUR - Level\n",
      "  ADF  : stat=-1.49, lag=34, p=0.5383\n",
      "  KPSS: stat=0.79, lag=21, p=0.0100\n",
      "\n",
      "EUR - First Difference\n",
      "  ADF  : stat=-3.12, lag=33, p=0.0253\n",
      "  KPSS: stat=0.43, lag=37, p=0.0655\n",
      "\n",
      "JPY - Level\n",
      "  ADF  : stat=-1.66, lag=22, p=0.4506\n",
      "  KPSS: stat=1.56, lag=21, p=0.0100\n",
      "\n",
      "JPY - First Difference\n",
      "  ADF  : stat=-4.09, lag=21, p=0.0010\n",
      "  KPSS: stat=0.20, lag=66, p=0.1000\n",
      "\n",
      "CNY - Level\n",
      "  ADF  : stat=-1.44, lag=32, p=0.5617\n",
      "  KPSS: stat=1.72, lag=21, p=0.0100\n",
      "\n",
      "CNY - First Difference\n",
      "  ADF  : stat=-3.12, lag=33, p=0.0248\n",
      "  KPSS: stat=0.10, lag=17, p=0.1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adf_lags = {\n",
    "    'BRL': {'level': 3,  'diff': 2},\n",
    "    'EUR': {'level': 34, 'diff': 33},\n",
    "    'JPY': {'level': 22, 'diff': 21},\n",
    "    'CNY': {'level': 32, 'diff': 33}\n",
    "}\n",
    "\n",
    "kpss_lags = {\n",
    "    'BRL': {'level': 21, 'diff': 4},\n",
    "    'EUR': {'level': 21, 'diff': 37},\n",
    "    'JPY': {'level': 21, 'diff': 66},\n",
    "    'CNY': {'level': 21, 'diff': 17}\n",
    "}\n",
    "\n",
    "def run_lag(series, name, level, adf_lag, kpss_lag):\n",
    "    data = series.dropna()\n",
    "    if level == 'diff':\n",
    "        data = data.diff().dropna()\n",
    "    \n",
    "    adf_result = adfuller(data, maxlag=adf_lag, autolag=None)\n",
    "    adf_stat = adf_result[0]\n",
    "    adf_pvalue = adf_result[1]\n",
    "\n",
    "    kpss_result = kpss(data, regression='c', nlags=kpss_lag)\n",
    "    kpss_stat = kpss_result[0]\n",
    "    kpss_pvalue = kpss_result[1]\n",
    "\n",
    "    return {\n",
    "        'name': name,\n",
    "        'level': 'First Difference' if level == 'diff' else 'Level',\n",
    "        'ADF': {'stat': adf_stat, 'lag': adf_lag, 'p': adf_pvalue},\n",
    "        'KPSS': {'stat': kpss_stat, 'lag': kpss_lag, 'p': kpss_pvalue}\n",
    "    }\n",
    "\n",
    "currencies = {'BRL': brl, 'EUR': eur, 'JPY': jpy, 'CNY': cny}\n",
    "results = []\n",
    "\n",
    "for name, df in currencies.items():\n",
    "    for level in ['level', 'diff']:\n",
    "        result = run_lag(\n",
    "            series=df['Close'],\n",
    "            name=name,\n",
    "            level=level,\n",
    "            adf_lag=adf_lags[name][level],\n",
    "            kpss_lag=kpss_lags[name][level]\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "for res in results:\n",
    "    print(f\"{res['name']} - {res['level']}\")\n",
    "    print(f\"  ADF  : stat={res['ADF']['stat']:.2f}, lag={res['ADF']['lag']}, p={res['ADF']['p']:.4f}\")\n",
    "    print(f\"  KPSS: stat={res['KPSS']['stat']:.2f}, lag={res['KPSS']['lag']}, p={res['KPSS']['p']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52a012c",
   "metadata": {},
   "source": [
    "The results remain similar to those originally obtained in the article, indicating that the bubbles observed in NFTs are more speculative than rational"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
